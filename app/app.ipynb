{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio==4.26.0 in /Users/yida/.pyenv/versions/3.10.4/envs/cds/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (4.26.0)\n",
      "Requirement already satisfied: fairscale==0.4.4 in /Users/yida/.pyenv/versions/3.10.4/envs/cds/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.4.4)\n",
      "Requirement already satisfied: timm in /Users/yida/.pyenv/versions/3.10.4/envs/cds/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.9.16)\n",
      "Collecting ast (from -r requirements.txt (line 4))\n",
      "  Using cached AST-0.0.2.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[8 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/qf/ngth87sn0_j9_2j6hl6_jlz40000gn/T/pip-install-mp4pf830/ast_2a03d15c3a30452395348b0dcc447009/setup.py\", line 6, in <module>\n",
      "  \u001b[31m   \u001b[0m     README = codecs.open(os.path.join(here, 'AST/README'), encoding='utf8').read()\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/yida/.pyenv/versions/3.10.4/lib/python3.10/codecs.py\", line 905, in open\n",
      "  \u001b[31m   \u001b[0m     file = builtins.open(filename, mode, buffering)\n",
      "  \u001b[31m   \u001b[0m FileNotFoundError: [Errno 2] No such file or directory: '/private/var/folders/qf/ngth87sn0_j9_2j6hl6_jlz40000gn/T/pip-install-mp4pf830/ast_2a03d15c3a30452395348b0dcc447009/AST/README'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.functional import pairwise_distance\n",
    "from torchvision.transforms.functional import InterpolationMode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load BLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape position embedding from 196 to 1024\n",
      "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('BLIP')\n",
    "from BLIP.models.blip import blip_feature_extractor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_size = 512\n",
    "blip_model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth'\n",
    "blip_model = blip_feature_extractor(pretrained=blip_model_url, image_size=image_size, vit='base', med_config='BLIP/configs/med_config.json')\n",
    "blip_model.eval()\n",
    "blip_model = blip_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blip_load_image(image_input,image_size,device):\n",
    "\n",
    "    raw_image = image_input.convert('RGB')   \n",
    "\n",
    "    w,h = raw_image.size\n",
    "#     display(raw_image.resize((w//5,h//5)))\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "        ]) \n",
    "    image = transform(raw_image).unsqueeze(0).to(device)   \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=128):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(256, embedding_size),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseNetwork(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "weights_path = '5_epoch_3linear_dropout_lr_5.pt'\n",
    "siamese_model = SiameseNetwork(input_size=768, embedding_size=128).to(device)\n",
    "siamese_model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "siamese_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_images = pd.read_csv('all_blip_siamese.csv')\n",
    "# set image_path as index\n",
    "df_all_images = df_all_images.set_index('image_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>group</th>\n",
       "      <th>blip</th>\n",
       "      <th>blip_siamese</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_path</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sofas/sofa-beds/91503_image_0_resized.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>453</td>\n",
       "      <td>[0.38693681359291077, 0.19435164332389832, -0....</td>\n",
       "      <td>[0.01813609153032303, -0.14332135021686554, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sofas/4-seaters-up/94024_image_0_resized.jpg</th>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>[0.6286948919296265, -0.5614968538284302, -0.1...</td>\n",
       "      <td>[0.22950240969657898, -0.12034033983945847, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sofas/l-shape/91717_image_0_resized.jpg</th>\n",
       "      <td>2</td>\n",
       "      <td>185</td>\n",
       "      <td>[0.42798036336898804, 0.20924703776836395, 0.0...</td>\n",
       "      <td>[0.09146980941295624, -0.23594890534877777, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sofas/4-seaters-up/88163_image_0_resized.jpg</th>\n",
       "      <td>3</td>\n",
       "      <td>115</td>\n",
       "      <td>[0.021315090358257294, 0.19990620017051697, -0...</td>\n",
       "      <td>[0.23008421063423157, -0.1584370881319046, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chairs/office-chairs/25435_image_0_resized.jpg</th>\n",
       "      <td>4</td>\n",
       "      <td>811</td>\n",
       "      <td>[0.2223481684923172, 0.12761016190052032, -0.0...</td>\n",
       "      <td>[0.19579604268074036, -0.28381842374801636, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Unnamed: 0  group  \\\n",
       "image_path                                                          \n",
       "sofas/sofa-beds/91503_image_0_resized.jpg                0    453   \n",
       "sofas/4-seaters-up/94024_image_0_resized.jpg             1    104   \n",
       "sofas/l-shape/91717_image_0_resized.jpg                  2    185   \n",
       "sofas/4-seaters-up/88163_image_0_resized.jpg             3    115   \n",
       "chairs/office-chairs/25435_image_0_resized.jpg           4    811   \n",
       "\n",
       "                                                                                             blip  \\\n",
       "image_path                                                                                          \n",
       "sofas/sofa-beds/91503_image_0_resized.jpg       [0.38693681359291077, 0.19435164332389832, -0....   \n",
       "sofas/4-seaters-up/94024_image_0_resized.jpg    [0.6286948919296265, -0.5614968538284302, -0.1...   \n",
       "sofas/l-shape/91717_image_0_resized.jpg         [0.42798036336898804, 0.20924703776836395, 0.0...   \n",
       "sofas/4-seaters-up/88163_image_0_resized.jpg    [0.021315090358257294, 0.19990620017051697, -0...   \n",
       "chairs/office-chairs/25435_image_0_resized.jpg  [0.2223481684923172, 0.12761016190052032, -0.0...   \n",
       "\n",
       "                                                                                     blip_siamese  \n",
       "image_path                                                                                         \n",
       "sofas/sofa-beds/91503_image_0_resized.jpg       [0.01813609153032303, -0.14332135021686554, -0...  \n",
       "sofas/4-seaters-up/94024_image_0_resized.jpg    [0.22950240969657898, -0.12034033983945847, -0...  \n",
       "sofas/l-shape/91717_image_0_resized.jpg         [0.09146980941295624, -0.23594890534877777, -0...  \n",
       "sofas/4-seaters-up/88163_image_0_resized.jpg    [0.23008421063423157, -0.1584370881319046, -0....  \n",
       "chairs/office-chairs/25435_image_0_resized.jpg  [0.19579604268074036, -0.28381842374801636, 0....  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_pad_image(image):\n",
    "    try:\n",
    "        img = image\n",
    "        scaling_factor = 512.0 / max(img.size)\n",
    "        new_size = (int(img.size[0] * scaling_factor), int(img.size[1] * scaling_factor))\n",
    "        img_resized = img.resize(new_size, Image.LANCZOS)\n",
    "        new_img = Image.new(\"RGB\", (512, 512), \"white\")\n",
    "        new_img.paste(img_resized, ((512 - new_size[0]) // 2, (512 - new_size[1]) // 2))\n",
    "        return new_img\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input_image(input_image):\n",
    "    resized_image = resize_pad_image(input_image)\n",
    "    blip_image = blip_load_image(resized_image, 512, device)\n",
    "    blip_embedding = blip_model(blip_image, '', mode='image')[0,0].tolist()\n",
    "    siamese_embedding = siamese_model(torch.tensor(blip_embedding).float().to(device)).detach()\n",
    "    return siamese_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_embeddings = torch.stack([torch.tensor(ast.literal_eval(embedding)) for embedding in df_all_images['blip_siamese']])\n",
    "\n",
    "def predict_similar_furniture(embedding):\n",
    "    siamese_distances = pairwise_distance(embedding, siamese_embeddings)\n",
    "    \n",
    "    _, top_5_indices_siamese = torch.topk(siamese_distances, 5, largest=False)\n",
    "    top_5_image_paths_siamese = df_all_images.iloc[top_5_indices_siamese.cpu().tolist()].index.tolist()\n",
    "    top_5_image_paths_siamese = [f\"../training-evaluation/all/{image_path}\" for image_path in top_5_image_paths_siamese]\n",
    "    \n",
    "    return top_5_image_paths_siamese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7889\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7889/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def recommend_similar_furniture(input_image):\n",
    "    if isinstance(input_image, np.ndarray):\n",
    "        input_image = Image.fromarray(input_image)\n",
    "    siamese_embedding = process_input_image(input_image)\n",
    "    similar_images = predict_similar_furniture(siamese_embedding)\n",
    "    print(similar_images)\n",
    "    return similar_images\n",
    "\n",
    "output_images = []\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        input_image = gr.Image(type='pil', label='Upload Image', height=512)\n",
    "    with gr.Row():\n",
    "        for i in range(3):\n",
    "            with gr.Column():\n",
    "                text = gr.Label(f'Top {i+1}')\n",
    "                output_image = gr.Image(type='pil', label=f'Top {i+1}', width=512)\n",
    "                output_images.append(output_image)\n",
    "    with gr.Row():\n",
    "        for i in range(2):\n",
    "            with gr.Column():\n",
    "                text = gr.Label(f'Top {i+4}')\n",
    "                output_image = gr.Image(type='pil', label=f'Top {i+4}', width=512)\n",
    "                output_images.append(output_image)\n",
    "        \n",
    "    input_image.change(fn=recommend_similar_furniture, inputs=[input_image], outputs=output_images)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
