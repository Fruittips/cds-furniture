{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Stratified Splitting of the Dataset\n",
    "Load json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcategory_to_category = {\n",
    "    '2-seaters':'sofas',\n",
    "    '3-seaters': 'sofas',\n",
    "    '4-seaters-up': 'sofas',\n",
    "    'sofa-beds': 'sofas',\n",
    "    'armchairs': 'sofas',\n",
    "    'l-shape': 'sofas',\n",
    "    'leather-sofas': 'sofas',\n",
    "    'lounge-chairs': 'sofas',\n",
    "    'outdoor-sofas': 'sofas',\n",
    "    'recliners': 'sofas',\n",
    "    'sofa-beds': 'sofas',\n",
    "    'sofa-sets': 'sofas',\n",
    "    'bar-stools': 'chairs',\n",
    "    'bean-bags-poufs': 'chairs',\n",
    "    'benches': 'chairs',\n",
    "    'dining-benches': 'chairs',\n",
    "    'dining-chairs': 'chairs',\n",
    "    'office-chairs': 'chairs',\n",
    "    'outdoor-dining-sets': 'chairs',\n",
    "    'stools-ottomans': 'chairs',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qf/ngth87sn0_j9_2j6hl6_jlz40000gn/T/ipykernel_17341/574487203.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subcategory</th>\n",
       "      <th>product_id</th>\n",
       "      <th>group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-seaters</td>\n",
       "      <td>54312</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-seaters</td>\n",
       "      <td>52072</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2-seaters</td>\n",
       "      <td>91532</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2-seaters</td>\n",
       "      <td>54882</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2-seaters</td>\n",
       "      <td>92674</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subcategory product_id group_id\n",
       "0   2-seaters      54312       13\n",
       "1   2-seaters      52072       13\n",
       "2   2-seaters      91532       13\n",
       "3   2-seaters      54882       13\n",
       "4   2-seaters      92674       13"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('manual-clusters-sofas-chairs.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "records = []\n",
    "\n",
    "for subcategory, products in data.items():\n",
    "    for product_id, group_id in products.items():\n",
    "        records.append({\n",
    "            \"subcategory\": subcategory,\n",
    "            \"product_id\": product_id,\n",
    "            \"group_id\": group_id\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df['product_id'] = df['product_id'].astype(str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`manual-clusters-sofas-chairs.json` contains products which do not have images, need to filter and drop them from the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2620\n",
      "2288\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def image_exists(subcategory, product_id, base_dir='all'):\n",
    "    subcategory_path = os.path.join(base_dir, subcategory_to_category[subcategory], subcategory)\n",
    "    for filename in os.listdir(subcategory_path):\n",
    "        if filename.startswith(f\"{product_id}_\") and filename.endswith(\"_resized.jpg\"):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "print(df.shape[0])\n",
    "df['image_exists'] = df.apply(lambda row: image_exists(row['subcategory'], row['product_id'], base_dir='all'), axis=1)\n",
    "df_filtered = df[df['image_exists']]\n",
    "df_filtered = df_filtered.drop(columns=['image_exists'])\n",
    "df_filtered.head()\n",
    "print(df_filtered.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out products without the primary image (index 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Base directory where your images are stored\n",
    "base_dir = 'all'\n",
    "\n",
    "# Initialize an empty set to store product IDs with a primary image\n",
    "product_ids_with_primary_image = set()\n",
    "\n",
    "# Walk through the file system\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if \"_image_0_resized.jpg\" in file:\n",
    "            # Extract product ID from filename\n",
    "            product_id = file.split('_')[0]\n",
    "            product_ids_with_primary_image.add(product_id)\n",
    "            \n",
    "# Filter the DataFrame\n",
    "df_filtered = df_filtered[df_filtered['product_id'].isin(product_ids_with_primary_image)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out groups with one product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2287\n",
      "2274\n"
     ]
    }
   ],
   "source": [
    "group_counts = df_filtered['group_id'].value_counts()\n",
    "\n",
    "valid_groups = group_counts[group_counts > 1].index\n",
    "print(df_filtered.shape[0])\n",
    "df_filtered = df_filtered[df_filtered['group_id'].isin(valid_groups)]\n",
    "print(df_filtered.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset. 80% training, 10% for testing, 10% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered['group_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not have enough group ids to do stratify splitting with scikit train_test_split \\n\n",
    "1. Ensure train dataset has one product from each group first\n",
    "2. Perform random splitting for the rest\n",
    "3. Verify distribution after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 2274\n",
      "Training set size: 1819 (79.99%)\n",
      "Validation set size: 227 (9.98%)\n",
      "Test set size: 228 (10.03%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df_initial = pd.DataFrame()\n",
    "\n",
    "groups = df_filtered['group_id'].unique()\n",
    "\n",
    "for group in groups:\n",
    "    group_subset = df_filtered[df_filtered['group_id'] == group]\n",
    "    train_df_initial = pd.concat([train_df_initial, group_subset.sample(n=2, random_state=42)])\n",
    "\n",
    "total_dataset_size = len(df_filtered)\n",
    "desired_training_size = total_dataset_size * 0.8\n",
    "additional_training_needed = max(0, desired_training_size - len(train_df_initial))\n",
    "\n",
    "df_remaining = df_filtered.drop(train_df_initial.index)\n",
    "split_size_for_remaining = (len(df_remaining) - additional_training_needed) / len(df_remaining)\n",
    "\n",
    "if additional_training_needed > 0:\n",
    "    additional_train_df, df_remaining = train_test_split(df_remaining, test_size=(split_size_for_remaining), random_state=42)\n",
    "    train_df = pd.concat([train_df_initial, additional_train_df], ignore_index=True)\n",
    "else:\n",
    "    train_df = train_df_initial.copy()\n",
    "\n",
    "validation_df, test_df = train_test_split(df_remaining, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Total dataset size: {total_dataset_size}\")\n",
    "print(f\"Training set size: {len(train_df)} ({(len(train_df)/total_dataset_size)*100:.2f}%)\")\n",
    "print(f\"Validation set size: {len(validation_df)} ({(len(validation_df)/total_dataset_size)*100:.2f}%)\")\n",
    "print(f\"Test set size: {len(test_df)} ({(len(test_df)/total_dataset_size)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure no overlap of product ids between datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap between training and validation sets: 0 products\n",
      "Overlap between training and testing sets: 0 products\n",
      "Overlap between validation and testing sets: 0 products\n"
     ]
    }
   ],
   "source": [
    "train_product_ids = set(train_df['image_path'])\n",
    "validation_product_ids = set(validation_df['image_path'])\n",
    "test_product_ids = set(test_df['image_path'])\n",
    "\n",
    "train_validation_overlap = train_product_ids.intersection(validation_product_ids)\n",
    "train_test_overlap = train_product_ids.intersection(test_product_ids)\n",
    "validation_test_overlap = validation_product_ids.intersection(test_product_ids)\n",
    "\n",
    "print(f\"Overlap between training and validation sets: {len(train_validation_overlap)} products\")\n",
    "print(f\"Overlap between training and testing sets: {len(train_test_overlap)} products\")\n",
    "print(f\"Overlap between validation and testing sets: {len(validation_test_overlap)} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking entropy scores to identify diverstiy and uniformity within each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.832410477296748 4.580859369951971 4.529220633633233\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "train_distribution = train_df['group_id'].value_counts()\n",
    "validation_distribution = validation_df['group_id'].value_counts()\n",
    "test_distribution = test_df['group_id'].value_counts()\n",
    "\n",
    "train_entropy = entropy(train_distribution.values)\n",
    "validation_entropy = entropy(validation_distribution.values)\n",
    "test_entropy = entropy(test_distribution.values)\n",
    "\n",
    "print(train_entropy, validation_entropy, test_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups with less than 2 products in the training set:\n",
      " Series([], Name: count, dtype: int64)\n",
      "\n",
      "Number of groups with at less than 2 products in the training set: 0\n"
     ]
    }
   ],
   "source": [
    "groups_with_less_than_2_products = train_distribution[train_distribution < 2]\n",
    "\n",
    "print(\"Groups with less than 2 products in the training set:\\n\", groups_with_less_than_2_products)\n",
    "print(\"\\nNumber of groups with at less than 2 products in the training set:\", len(groups_with_less_than_2_products))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify split ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1819 227 228\n",
      "Training set ratio: 0.80\n",
      "Validation set ratio: 0.10\n",
      "Test set ratio: 0.10\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape[0], validation_df.shape[0], test_df.shape[0])\n",
    "total_instances = len(train_df) + len(validation_df) + len(test_df)\n",
    "train_ratio = len(train_df) / total_instances\n",
    "validation_ratio = len(validation_df) / total_instances\n",
    "test_ratio = len(test_df) / total_instances\n",
    "\n",
    "print(f\"Training set ratio: {train_ratio:.2f}\")\n",
    "print(f\"Validation set ratio: {validation_ratio:.2f}\")\n",
    "print(f\"Test set ratio: {test_ratio:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original distribution\n",
    "original_distribution = df_filtered['group_id'].value_counts(normalize=True)\n",
    "\n",
    "# Training distribution\n",
    "train_distribution = train_df['group_id'].value_counts(normalize=True)\n",
    "\n",
    "# Validation distribution\n",
    "validation_distribution = validation_df['group_id'].value_counts(normalize=True)\n",
    "\n",
    "# Test distribution\n",
    "test_distribution = test_df['group_id'].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Copy images over to their respective folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_images(df, dest_dir, source_dir='all'):\n",
    "    if dest_dir == None or dest_dir == '':\n",
    "        raise ValueError(\"Please provide a destination directory.\")\n",
    "\n",
    "    dest_path = dest_dir\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        subcategory = row['subcategory']\n",
    "        product_id = str(row['product_id'])\n",
    "        \n",
    "        src_path = os.path.join(source_dir, subcategory_to_category[subcategory], subcategory, f\"{product_id}_image_0_resized.jpg\")\n",
    "        dst_path = os.path.join(dest_path,  subcategory_to_category[subcategory], subcategory, f\"{product_id}_image_0_resized.jpg\")\n",
    "        \n",
    "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "        \n",
    "        shutil.copy2(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_images_dir = 'all'\n",
    "# train_images_dir = 'training'\n",
    "# validation_images_dir = 'validation'\n",
    "# test_images_dir = 'test'\n",
    "\n",
    "# copy_images(train_df, source_dir=source_images_dir, dest_dir=train_images_dir)\n",
    "# copy_images(validation_df, source_dir=source_images_dir, dest_dir=validation_images_dir)\n",
    "# copy_images(test_df, source_dir=source_images_dir, dest_dir=test_images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataframes into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>sofas/2-seaters/91513_image_0_resized.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>sofas/2-seaters/87437_image_0_resized.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>sofas/2-seaters/88716_image_0_resized.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>sofas/2-seaters/94018_image_0_resized.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>sofas/2-seaters/91579_image_0_resized.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group                                 image_path\n",
       "0    13  sofas/2-seaters/91513_image_0_resized.jpg\n",
       "1    13  sofas/2-seaters/87437_image_0_resized.jpg\n",
       "2    14  sofas/2-seaters/88716_image_0_resized.jpg\n",
       "3    14  sofas/2-seaters/94018_image_0_resized.jpg\n",
       "4    15  sofas/2-seaters/91579_image_0_resized.jpg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>sofas/outdoor-sofas/14619_image_0_resized.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>sofas/3-seaters/46964_image_0_resized.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61</td>\n",
       "      <td>sofas/3-seaters/92228_image_0_resized.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332</td>\n",
       "      <td>sofas/lounge-chairs/49322_image_0_resized.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>879</td>\n",
       "      <td>chairs/stools-ottomans/41477_image_0_resized.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group                                        image_path\n",
       "0   391     sofas/outdoor-sofas/14619_image_0_resized.jpg\n",
       "1    93         sofas/3-seaters/46964_image_0_resized.jpg\n",
       "2    61         sofas/3-seaters/92228_image_0_resized.jpg\n",
       "3   332     sofas/lounge-chairs/49322_image_0_resized.jpg\n",
       "4   879  chairs/stools-ottomans/41477_image_0_resized.jpg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>453</td>\n",
       "      <td>sofas/sofa-beds/91503_image_0_resized.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104</td>\n",
       "      <td>sofas/4-seaters-up/94024_image_0_resized.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>185</td>\n",
       "      <td>sofas/l-shape/91717_image_0_resized.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115</td>\n",
       "      <td>sofas/4-seaters-up/88163_image_0_resized.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>811</td>\n",
       "      <td>chairs/office-chairs/25435_image_0_resized.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group                                      image_path\n",
       "0   453       sofas/sofa-beds/91503_image_0_resized.jpg\n",
       "1   104    sofas/4-seaters-up/94024_image_0_resized.jpg\n",
       "2   185         sofas/l-shape/91717_image_0_resized.jpg\n",
       "3   115    sofas/4-seaters-up/88163_image_0_resized.jpg\n",
       "4   811  chairs/office-chairs/25435_image_0_resized.jpg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save_csv(df, filename):\n",
    "    df['image_path'] = df.apply(lambda row: f\"{subcategory_to_category[row['subcategory']]}/{row['subcategory']}/{row['product_id']}_image_0_resized.jpg\", axis=1)\n",
    "    df.drop(columns=['subcategory', 'product_id'], inplace=True)\n",
    "    df.rename(columns={'group_id': 'group'}, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    display(df.head())\n",
    "    df.to_csv(f'new_images_csv/{filename}', index=False)\n",
    "\n",
    "save_csv(train_df, 'training_images.csv')\n",
    "save_csv(validation_df, 'validation_images.csv')\n",
    "save_csv(test_df, 'testing_images.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generation of triplet pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('new_images_csv/training_images.csv')\n",
    "validation_df = pd.read_csv('new_images_csv/validation_images.csv')\n",
    "test_df = pd.read_csv('new_images_csv/testing_images.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using simple generation by sampling rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "from math import factorial\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "def calculate_permutations(n, k):\n",
    "    return factorial(n) // factorial(n - k)\n",
    "\n",
    "def ensure_representation_and_all_orderings_triplets(dataset, max_rows=None):\n",
    "    grouped_data = dataset.groupby('group')['image_path'].apply(list).to_dict()\n",
    "    all_groups = list(grouped_data.keys())\n",
    "    no_of_triplets_per_group = max_rows // len(all_groups) if max_rows else None\n",
    "    no_of_triplets_generated_per_group = {group: 0 for group in all_groups}\n",
    "    \n",
    "    if not max_rows:\n",
    "        max_triplets = 0\n",
    "        for images in grouped_data.values():\n",
    "            if len(images) >= 2:\n",
    "                n = len(images)\n",
    "                k = 2\n",
    "                max_triplets += calculate_permutations(n, k) * (len(dataset) - n)\n",
    "        max_rows = int(max_triplets)\n",
    "    \n",
    "    triplets = []\n",
    "\n",
    "    for group in all_groups:\n",
    "        for other_group in all_groups:\n",
    "            if group == other_group:\n",
    "                continue\n",
    "            positive_images = grouped_data[group]\n",
    "            negative_images = grouped_data[other_group]\n",
    "            \n",
    "            if len(positive_images) < 2 or not negative_images:\n",
    "                continue \n",
    "            \n",
    "            A1, A2 = positive_images[:2]\n",
    "            B = negative_images[0]\n",
    "            triplets.append((A1, A2, B, group, other_group))\n",
    "            triplets.append((A2, A1, B, group, other_group))\n",
    "            no_of_triplets_generated_per_group[group] += 1\n",
    "            \n",
    "            if len(triplets) >= max_rows:\n",
    "                return triplets\n",
    "\n",
    "    for group in all_groups:\n",
    "        positive_images = grouped_data[group]\n",
    "        shuffle(positive_images)\n",
    "        for A1, A2 in permutations(positive_images, 2):\n",
    "            if no_of_triplets_per_group and no_of_triplets_generated_per_group[group] >= no_of_triplets_per_group:\n",
    "                break\n",
    "            \n",
    "            for other_group in all_groups:\n",
    "                if group == other_group:\n",
    "                    continue\n",
    "                \n",
    "                negative_images = grouped_data[other_group]\n",
    "                shuffle(negative_images)\n",
    "                \n",
    "                for B in negative_images:\n",
    "                    triplets.append((A1, A2, B, group, other_group))\n",
    "                    triplets.append((A2, A1, B, group, other_group))\n",
    "                    no_of_triplets_generated_per_group[group] += 2\n",
    "                    \n",
    "                    if no_of_triplets_per_group and no_of_triplets_generated_per_group[group] >= no_of_triplets_per_group:\n",
    "                        break\n",
    "                    \n",
    "                    if len(triplets) >= max_rows:\n",
    "                        return triplets\n",
    "    \n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21978\n",
      "7415056\n",
      "4953630\n"
     ]
    }
   ],
   "source": [
    "train_triplets = ensure_representation_and_all_orderings_triplets(train_df, max_rows=10000000)\n",
    "triplets_train_df = pd.DataFrame(train_triplets, columns=[\"anchor\", \"similar\", \"dissimilar\", \"similar_group\", \"dissimilar_group\"])\n",
    "print(triplets_train_df.shape[0])\n",
    "triplets_train_df.drop_duplicates(subset=['anchor', 'similar', 'dissimilar'], inplace=True)\n",
    "print(triplets_train_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1088\n",
      "495364\n",
      "485614\n"
     ]
    }
   ],
   "source": [
    "# reduced training set for hyperparameter tuning ~10% of the original training size\n",
    "max_no_of_triplets = round(0.1 * triplets_train_df.shape[0])\n",
    "reduced_train_triplets = ensure_representation_and_all_orderings_triplets(train_df, max_rows=max_no_of_triplets)\n",
    "triplets_reduced_train_df = pd.DataFrame(reduced_train_triplets, columns=[\"anchor\", \"similar\", \"dissimilar\", \"similar_group\", \"dissimilar_group\"])\n",
    "print(triplets_reduced_train_df.shape[0])\n",
    "triplets_reduced_train_df.drop_duplicates(subset=['anchor', 'similar', 'dissimilar'], inplace=True)\n",
    "print(triplets_reduced_train_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 455, 455)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure that training triplets and reduced training triplets set has all groups as well\n",
    "train_df['group'].nunique(), triplets_train_df['similar_group'].nunique(), triplets_reduced_train_df['similar_group'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_reduced_train_df = triplets_reduced_train_df.sample(frac=1).reset_index(drop=True)\n",
    "triplets_reduced_train_df.to_csv('new_images_csv/reduced_training_triplets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_train_df = triplets_train_df.sample(frac=1).reset_index(drop=True)\n",
    "triplets_train_df.to_csv('new_images_csv/training_triplets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_train_df = triplets_train_df.sample(frac=1).reset_index(drop=True)\n",
    "triplets_train_df.to_csv('new_images_csv/training_triplets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 3645\n"
     ]
    }
   ],
   "source": [
    "total_rows = 116614\n",
    "batch_size = 32\n",
    "steps_per_epoch = total_rows // batch_size\n",
    "\n",
    "# If you want to include the last partial batch in the count, you can do the following instead:\n",
    "import math\n",
    "steps_per_epoch = math.ceil(total_rows / batch_size)\n",
    "\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if triplet pairs missing anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455 455\n",
      "Jensen-Shannon Divergence between distribution of triplet pairs and original dataset: 0.03763558725148959\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def jensen_shannon_divergence(p, q):\n",
    "    \"\"\"\n",
    "    Compute Jensen-Shannon Divergence between two probability distributions.\n",
    "    \"\"\"\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "    m = 0.5 * (p + q)\n",
    "    jsd = 0.5 * (np.sum(p * np.log2(p / m)) + np.sum(q * np.log2(q / m)))\n",
    "    return jsd\n",
    "\n",
    "print(triplets_train_df['similar_group'].nunique(), train_df['group'].nunique())\n",
    "\n",
    "triplet_group_distribution = triplets_train_df['similar_group'].value_counts(normalize=True)\n",
    "original_group_distribution = train_df['group'].value_counts(normalize=True)\n",
    "\n",
    "jsd_score = jensen_shannon_divergence(triplet_group_distribution, original_group_distribution)\n",
    "\n",
    "print(\"Jensen-Shannon Divergence between distribution of triplet pairs and original dataset:\", jsd_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jensen-Shannon Divergence between distribution of triplet pairs and original dataset: 0.05404983284481284\n"
     ]
    }
   ],
   "source": [
    "common_groups = set(test_df['group'].unique()).intersection(set(triplets_reduced_train_df['similar_group'].unique()))\n",
    "# Convert the set of common groups to a list\n",
    "common_groups_list = list(common_groups)\n",
    "\n",
    "# Filter distributions to include only common groups\n",
    "filtered_distribution_original = original_group_distribution.loc[common_groups_list]\n",
    "filtered_distribution_triplets = triplet_group_distribution.loc[common_groups_list]\n",
    "\n",
    "# Normalize distributions if not already normalized\n",
    "filtered_distribution_original /= filtered_distribution_original.sum()\n",
    "filtered_distribution_triplets /= filtered_distribution_triplets.sum()\n",
    "\n",
    "# Calculate JSD\n",
    "jsd_score = jensen_shannon_divergence(filtered_distribution_original, filtered_distribution_triplets)\n",
    "print(\"Jensen-Shannon Divergence between distribution of triplet pairs and original dataset:\", jsd_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
